{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional models\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103d32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100000, 31)\n",
      "   age  gender ethnicity education_level  income_level employment_status  \\\n",
      "0   58    Male     Asian      Highschool  Lower-Middle          Employed   \n",
      "1   48  Female     White      Highschool        Middle          Employed   \n",
      "2   60    Male  Hispanic      Highschool        Middle        Unemployed   \n",
      "3   74  Female     Black      Highschool           Low           Retired   \n",
      "4   46    Male     White        Graduate        Middle           Retired   \n",
      "\n",
      "  smoking_status  alcohol_consumption_per_week  \\\n",
      "0          Never                             0   \n",
      "1         Former                             1   \n",
      "2          Never                             1   \n",
      "3          Never                             0   \n",
      "4          Never                             1   \n",
      "\n",
      "   physical_activity_minutes_per_week  diet_score  ...  hdl_cholesterol  \\\n",
      "0                                 215         5.7  ...               41   \n",
      "1                                 143         6.7  ...               55   \n",
      "2                                  57         6.4  ...               66   \n",
      "3                                  49         3.4  ...               50   \n",
      "4                                 109         7.2  ...               52   \n",
      "\n",
      "   ldl_cholesterol  triglycerides  glucose_fasting  glucose_postprandial  \\\n",
      "0              160            145              136                   236   \n",
      "1               50             30               93                   150   \n",
      "2               99             36              118                   195   \n",
      "3               79            140              139                   253   \n",
      "4              125            160              137                   184   \n",
      "\n",
      "   insulin_level  hba1c  diabetes_risk_score  diabetes_stage  \\\n",
      "0           6.36   8.18                 29.6          Type 2   \n",
      "1           2.00   5.63                 23.0     No Diabetes   \n",
      "2           5.07   7.51                 44.7          Type 2   \n",
      "3           5.28   9.03                 38.2          Type 2   \n",
      "4          12.74   7.20                 23.5          Type 2   \n",
      "\n",
      "   diagnosed_diabetes  \n",
      "0                   1  \n",
      "1                   0  \n",
      "2                   1  \n",
      "3                   1  \n",
      "4                   1  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/diabetes_dataset.csv\")  # rename if needed\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column candidates & dtypes:\n",
      "family_history_diabetes int64 [0 1]\n",
      "diabetes_risk_score float64 [29.6 23.  44.7 38.2 23.5 36.1 34.2 26.7 30.  25.3]\n",
      "diabetes_stage object ['Type 2' 'No Diabetes' 'Pre-Diabetes' 'Gestational' 'Type 1']\n",
      "diagnosed_diabetes int64 [1 0]\n",
      "y unique values: [1, 0]\n",
      "class counts:\n",
      " diagnosed_diabetes\n",
      "1    59998\n",
      "0    40002\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TARGET_COL = \"diagnosed_diabetes\"\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"'{TARGET_COL}' not found in columns: {list(df.columns)[:10]} ...\")\n",
    "\n",
    "# Coerce to 0/1 robustly in case dtype is float/bool\n",
    "y = df[TARGET_COL].astype(int).clip(0, 1)\n",
    "print(\"Class counts (0=no, 1=yes):\\n\", y.value_counts())\n",
    "\n",
    "# Drop target + any leakage cols if present\n",
    "leakage_like = {\"diabetes_risk_score\", \"diabetes_stage\", TARGET_COL}\n",
    "X = df.drop(columns=[c for c in leakage_like if c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578a3d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['diagnosed_diabetes'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiagnosed_diabetes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnosed_diabetes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n",
      "File \u001b[1;32mc:\\Users\\eddie\\anaconda3\\envs\\ml-fitness\\lib\\site-packages\\pandas\\core\\frame.py:5603\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5456\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5457\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5464\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5467\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5468\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5601\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5605\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5609\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5610\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eddie\\anaconda3\\envs\\ml-fitness\\lib\\site-packages\\pandas\\core\\generic.py:4810\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4810\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\eddie\\anaconda3\\envs\\ml-fitness\\lib\\site-packages\\pandas\\core\\generic.py:4852\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4850\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4851\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4852\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4853\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4855\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4856\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\eddie\\anaconda3\\envs\\ml-fitness\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7136\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7137\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['diagnosed_diabetes'] not found in axis\""
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e54b9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[0;32m     21\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m res_lr \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLogistic Regression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Random Forest\u001b[39;00m\n\u001b[0;32m     25\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(name, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model\u001b[39m(name, model):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m     probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eddie\\anaconda3\\envs\\ml-fitness\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\eddie\\anaconda3\\envs\\ml-fitness\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1335\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1333\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1339\u001b[0m     )\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1342\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)"
     ]
    }
   ],
   "source": [
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "def get_transformed_feature_names(fitted_preprocessor):\n",
    "    names = []\n",
    "    if num_cols:\n",
    "        names.extend(num_cols)\n",
    "    if cat_cols:\n",
    "        ohe = fitted_preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "        names.extend(list(ohe.get_feature_names_out(cat_cols)))\n",
    "    return np.array(names, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cb355",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# Logistic Regression (balanced)\n",
    "models.append((\n",
    "    \"LogisticRegression\",\n",
    "    Pipeline(steps=[\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\"\n",
    "        ))\n",
    "    ])\n",
    "))\n",
    "\n",
    "# Random Forest\n",
    "models.append((\n",
    "    \"RandomForest\",\n",
    "    Pipeline(steps=[\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=400, max_depth=None, n_jobs=-1,\n",
    "            class_weight=\"balanced_subsample\", random_state=42\n",
    "        ))\n",
    "    ])\n",
    "))\n",
    "\n",
    "# XGBoost (if available) — use scale_pos_weight for imbalance\n",
    "if HAS_XGB:\n",
    "    pos = (y_train == 1).sum()\n",
    "    neg = (y_train == 0).sum()\n",
    "    spw = max(1.0, neg / max(pos, 1))\n",
    "    models.append((\n",
    "        \"XGBoost\",\n",
    "        Pipeline(steps=[\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", XGBClassifier(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                eval_metric=\"logloss\",\n",
    "                tree_method=\"hist\",\n",
    "                scale_pos_weight=spw\n",
    "            ))\n",
    "        ])\n",
    "    ))\n",
    "else:\n",
    "    print(\"XGBoost not installed; skipping XGB model.\")\n",
    "\n",
    "# LightGBM (if available)\n",
    "if HAS_LGBM:\n",
    "    models.append((\n",
    "        \"LightGBM\",\n",
    "        Pipeline(steps=[\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"clf\", lgb.LGBMClassifier(\n",
    "                n_estimators=600,\n",
    "                learning_rate=0.05,\n",
    "                num_leaves=63,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                class_weight=\"balanced\"\n",
    "            ))\n",
    "        ])\n",
    "    ))\n",
    "else:\n",
    "    print(\"LightGBM not installed; skipping LGBM model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47efc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion(cm, title):\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[\"Pred 0\",\"Pred 1\"],\n",
    "                yticklabels=[\"True 0\",\"True 1\"])\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, pipe in models:\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    proba = None\n",
    "    if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(pipe.named_steps[\"clf\"], \"decision_function\"):\n",
    "        # map decision_function to [0,1] with a sigmoid-ish transform\n",
    "        dec = pipe.decision_function(X_test)\n",
    "        proba = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    roc = roc_auc_score(y_test, proba) if proba is not None else np.nan\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}  |  Precision: {prec:.4f}  |  Recall: {rec:.4f}  |  F1: {f1:.4f}  |  ROC-AUC: {roc:.4f}\")\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print_confusion(cm, f\"Confusion Matrix - {name}\")\n",
    "\n",
    "    # ROC curve\n",
    "    if proba is not None:\n",
    "        RocCurveDisplay.from_predictions(y_test, proba, name=name)\n",
    "        plt.title(f\"ROC Curve - {name}\")\n",
    "        plt.show()\n",
    "\n",
    "    results.append({\n",
    "        \"model\": name, \"accuracy\": acc, \"precision\": prec,\n",
    "        \"recall\": rec, \"f1\": f1, \"roc_auc\": roc\n",
    "    })\n",
    "\n",
    "# Summary table\n",
    "summary = pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False)\n",
    "print(\"\\n===== Model Comparison =====\")\n",
    "print(summary)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(x=\"model\", y=\"roc_auc\", data=summary)\n",
    "plt.title(\"ROC-AUC by Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27510d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = summary.iloc[0][\"model\"]\n",
    "best_pipe = dict(models)[best_name]\n",
    "print(f\"\\nComputing permutation importance for: {best_name}\")\n",
    "\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Get transformed feature names (sklearn >= 1.0)\n",
    "try:\n",
    "    feat_names = best_pipe.named_steps[\"prep\"].get_feature_names_out()\n",
    "except Exception:\n",
    "    # Fallback (older sklearn): build names manually\n",
    "    num_feats = np.array(num_cols, dtype=object)\n",
    "    if cat_cols:\n",
    "        ohe = best_pipe.named_steps[\"prep\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "        cat_feats = ohe.get_feature_names_out(cat_cols)\n",
    "        feat_names = np.concatenate([num_feats, cat_feats])\n",
    "    else:\n",
    "        feat_names = num_cols\n",
    "\n",
    "r = permutation_importance(\n",
    "    best_pipe, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": feat_names,\n",
    "    \"importance_mean\": r.importances_mean,\n",
    "    \"importance_std\": r.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False).head(20)\n",
    "\n",
    "print(\"\\nTop 20 permutation importances:\")\n",
    "print(imp_df)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(y=\"feature\", x=\"importance_mean\", data=imp_df)\n",
    "plt.title(f\"Top 20 Permutation Importances - {best_name}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e4d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa9919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10db3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad2e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-fitness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
